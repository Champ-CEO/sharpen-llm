{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, text: str):\n",
    "        self.char_to_idx = {}\n",
    "\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        self.build_vocab(text)\n",
    "\n",
    "    def build_vocab(self, text):\n",
    "        # Create sorted vocabulary from unique characters\n",
    "        unique_chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(unique_chars)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert string to list of indices\"\"\"\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of indices to string\"\"\"\n",
    "        return \"\".join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "    def encode_tensor(self, text):\n",
    "        \"\"\"Convert string to PyTorch tensor\"\"\"\n",
    "        return torch.tensor([self.encode(text)])\n",
    "\n",
    "    def decode_tensor(self, tensor):\n",
    "        \"\"\"Convert PyTorch tensor to string\"\"\"\n",
    "        return self.decode(tensor.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "I grew up on the crime side, the New York Times side\n",
    "Stayin' alive was no jive\n",
    "At second hands, moms bounced on old men\n",
    "So then we moved to Shaolin land\n",
    "\"\"\".strip()\n",
    "\n",
    "vocab = Vocabulary(text)\n",
    "vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 27, 10, 31, 17, 22, 2, 1, 10, 20, 17, 29, 14, 1, 30, 10, 26, 1, 22, 23, 1, 18, 17, 29, 14]\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Stayin' alive was no jive\"\n",
    "print(vocab.encode(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "ATTENTION_HEADS = 4\n",
    "FEED_FORWARD_SIZE = 128\n",
    "DROPOOUT = 0.1\n",
    "CONTEXT_WINDOW = 128\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_k = EMBEDDING_SIZE // ATTENTION_HEADS\n",
    "\n",
    "        self.q_linear = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.k_linear = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.v_linear = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.out = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(q)\n",
    "            .view(batch_size, -1, ATTENTION_HEADS, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(k)\n",
    "            .view(batch_size, -1, ATTENTION_HEADS, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(v)\n",
    "            .view(batch_size, -1, ATTENTION_HEADS, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, EMBEDDING_SIZE)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_SIZE, FEED_FORWARD_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOOUT),\n",
    "            nn.Linear(FEED_FORWARD_SIZE, EMBEDDING_SIZE),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.norm1 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "        self.norm2 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "        self.dropout = nn.Dropout(DROPOOUT)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attended = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attended))\n",
    "\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(fed_forward))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, CONTEXT_WINDOW, EMBEDDING_SIZE)\n",
    "        )\n",
    "        self.transformer = TransformerBlock()\n",
    "        self.fc = nn.Linear(EMBEDDING_SIZE, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) + self.pos_embedding[:, : x.size(1)]\n",
    "        x = self.transformer(x, mask)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sentence):\n",
    "    vocab = Vocabulary(sentence)\n",
    "\n",
    "    # Prepare input and target sequences\n",
    "    x = vocab.encode_tensor(sentence[:-1])  # Input sequence\n",
    "    y = vocab.encode_tensor(sentence[1:])  # Target sequence\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = Transformer(vocab.vocab_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.view(-1, vocab.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model, vocab\n",
    "\n",
    "\n",
    "def generate(model, prefix, vocab, max_new_chars=32):\n",
    "    model.eval()\n",
    "    current_sequence = vocab.encode(prefix)\n",
    "    result = prefix\n",
    "\n",
    "    for _ in range(max_new_chars):\n",
    "        # Predict next character\n",
    "        x = torch.tensor([current_sequence])\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            next_char_idx = torch.argmax(output[0, -1]).item()\n",
    "\n",
    "        # Add predicted character to sequence\n",
    "        current_sequence.append(next_char_idx)\n",
    "        result += vocab.idx_to_char[next_char_idx]\n",
    "\n",
    "        # Stop if we predict a period\n",
    "        if vocab.idx_to_char[next_char_idx] == \".\":\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.4024\n",
      "Epoch 200, Loss: 0.0857\n",
      "Epoch 300, Loss: 0.0362\n",
      "Epoch 400, Loss: 0.0203\n",
      "Epoch 500, Loss: 0.0143\n",
      "Epoch 600, Loss: 0.0096\n",
      "Epoch 700, Loss: 0.0072\n",
      "Epoch 800, Loss: 0.0057\n",
      "Epoch 900, Loss: 0.0046\n",
      "Epoch 1000, Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "model, vocab = train(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown\"\n",
    "generated = generate(model, text, vocab, max_new_chars=4)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
